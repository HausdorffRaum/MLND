{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "### Credit Card Fraud Detection\n",
    "Xueqiu Feng\n",
    "June 27th, 2018\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "- #### Problem Domain\n",
    "Wikipedia defines fraud as: *deliberate deception to secure unfair or unlawful gain, or to deprive a victim of a legal right*. Fraud or deliberate deception is a skill that every species has already mastered to perfection through evolution, which is especially true for human being. In the modern era, with the development of new technologies, new forms of fraud have also been invented. Although fraud appears to be rarely happening, it can result in an huge amount of loss. Therefore, fraud detection is a focal point for insurance, financial, retail and tele-communication sectors. It also has drawed attention from the academical \n",
    "\n",
    "- #### Project Origin\n",
    "I used the Kaggle dataset [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud), which was collected and analysed during a research collaboration of Worldline and the [Machine Learning Group of ULB (Universit√© Libre de Bruxelles)](http://mlg.ulb.ac.be) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML.\n",
    "\n",
    "- #### Dataset\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features $V_1, V_2, ... V_{28}$ are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Credit card is almost a necessity to survive in the modern society. However, it associates with risk exposure because of physical or informational theft. Whereas a stolen card can be reported and frozen immediately to prevent unauthorised transactions, a compromised account can be abused without notice untill receiving bill statement, where before the frudulent use the account information can be hold for an arbitrary time, making the source even harder to trace. Because of the huge amount of transactions, it is nearly impossible to check them one by one manuelly, which will also inevitablly result in inacceptable delay of transactions. \n",
    "\n",
    "It is important that, credit card companies are able to recognize fraudulent credit card transactions, so that customers are not charged for items that they did not purchase. Therefore it would be very meaningful to build a system which can automatically detect dubious transactions and freeze it for further inspection.\n",
    "\n",
    "The creditcard fraud detection problem can be easily formulated as a supervised machine learning problem, with the binary labels *fraud* or *non-fraud*, i.e. binary classification. Therefore almost all the possible supervised machine learning algorithms can be applied. Where the *AUC* and *F score* can be very relevant in terms of determining the quality of the classifier, for the reason that the dataset is strongly imbalanced.\n",
    "\n",
    "Neural network will not be considered because of the long training time and the difficulty of finding the best architecture. Moreover, my goal is to use the conventional machine learning algorithms to come as close as possible to be competitive against neural network, which will also be my benchmark model. At the end the classifiers will be compared such that the optimal one, depending on my own definition of optimality, will be chosen to tackle this particular problem.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Due to the imbalanced nature of this dataset and the binary of the classification itself, following metrics are applied:\n",
    "\n",
    "- #### F Score\n",
    "\n",
    "It is defined as \n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{\\beta} = (1+\\beta^2)\\frac{precisision \\cdot recall}{\\beta^2\\cdot precision + recall}\n",
    "\\end{equation}, \\quad\\beta\\in [0,+\\infty)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{equation}\n",
    "precision = \\frac{tp}{tp + fp}\n",
    "\\end{equation}\n",
    "$$\n",
    "i.e. out all all the transactions which are classified as fraud, the share of real fraud transactions.\n",
    "$$\n",
    "\\begin{equation}\n",
    "recall = \\frac{tp}{tp + fn}\n",
    "\\end{equation}\n",
    "$$\n",
    "i.e. the success rate of detection if a fraud transaction is being conducted.\n",
    "\n",
    "For our dataset, where the positive (frauds) class only account for 0.172% of all transactions, the measure of *recall* shall be much more important than *precision*. Because it would be preferable to identify non-fraud as fraud then the other way around, i.e. it might be sensible to pay the price that some of the non-fraud transactions are classified as fraud, in order to identify all the fraud transactions.\n",
    "\n",
    "However, in an extreme scenario where we assert indiscriminately that, all the samples are fraud, *recall* will be a perfect 100%, whereas *precision* will be nearly 0.\n",
    "\n",
    "Thus we will use the $F_{0.5}$ score to give *recall* more weight over *precision* in the evaluation.\n",
    "\n",
    "- #### Recall & Precision\n",
    "\n",
    "Alternatively we could also simply use *recall* and *precision* as metric.\n",
    "\n",
    "- #### Area under the Receiver Operating Characteristic Curve (AUROC)\n",
    "\n",
    "A *receiver operating characteristic curve*, i.e. *ROC curve*, is created by plotting *recall* against *false positive rate (fpr)*, which is defined as\n",
    "$$\n",
    "\\begin{equation}\n",
    "fpr = \\frac{fp}{fp + tn}\n",
    "\\end{equation}\n",
    "$$\n",
    "i.e. the ratio between the number of normal transactions wrongly categorized as fraud and the total number of actual normal transactions, with thresholds from 0 to 1.\n",
    "\n",
    "Which can be understood as how the classifier's performance varies with different thresholds, where the classifier has an output of probability predicting a transaction being fraud. And only if the probability passes the threshold, the transaction shall be categorized as fraud. Notice that this measure is therefore only suitable for classifiers that have probablistic outputs.\n",
    "\n",
    "The area under the *ROC curve* (often referred to as simply the *AUC* or *AUROC*) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative')<cite data-cite=\"5955268/AL6LRDFR\"></cite>.\n",
    "\n",
    "- #### False positive Rate\n",
    "\n",
    "False positive rate is also an useful metric, it indicates what is the proportion of normal transactions being classified as fraudulent. Since there is an huge number of transactions occur every minute, it would be undesirable to constantly freeze normal transactions for further inspections.\n",
    "\n",
    "\n",
    "##### Among the above mentioned metrics, I consider *AUC*, *recall* and *false positive rate* the most relevent metrics for evaluation the performance of classifiers for this problem.\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "- #### Original Data\n",
    "![](data.png)\n",
    "\n",
    "- #### Descriptive Statistics\n",
    "![](data_describe.png)\n",
    "Obviously, the 28 anonamised features are centered already. Whereas the feature 'Amount' is not only not centered, but has also a totally different scale, which means I have some data pre-processing work to do later on.\n",
    "\n",
    "- #### Missing Values\n",
    "![](na.png)\n",
    "There are no missing values in every feature, i.e. 'Time', V1, ..., V29, 'Amount' are missing value free.\n",
    "\n",
    "### Exploratory Visualization\n",
    "\n",
    "- #### Time\n",
    "![](tt.png)\n",
    "We observe that, the majority of transactions (non-fraud) were conducted periodically. There is also a vage pattern of fraud transactions, presumly occured during the night.\n",
    "\n",
    "- #### Amount\n",
    "![](aa.png)\n",
    "Clearly, the amounts need to be transformed such that there is more discrimination.\n",
    "\n",
    "### Algorithms and Techniques\n",
    "I aim to use simple and interpretable algorithms to tackle the problem of fraud detection, where at the end there are probability outputs of either class. Thus in my opion the following algorithms are potential candidates.\n",
    "\n",
    "- #### Decision Tree\n",
    "This is the most intuitive algorithm, although there are some very technical details about how the tree should grow exactly. Basically it is just as the same as our daily decision makings, where we ask ourselves a sequence of simple questions which can be answered with 'yes' or 'no'. At the end we will reach a decision depending on our answers.\n",
    "\n",
    "The algorithm was implemented with the default settings from sklearn except that the *random_state* is set to 0, i.e. *(criterion=‚Äôgini‚Äô, splitter=‚Äôbest‚Äô, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)*\n",
    "\n",
    "- #### Gaussian Naive Bayes\n",
    "Gaussian naive Bayes is a popular algorithm in classification problems with approximately Gaussian features, which is our case after some transformations. I will give it a chance and see if it can handle this job well. Generally, we assume that given the class, the features are independently Gaussian distributed. Thus given features $\\mathbf{x} = (x_1, ..., x_p)$ for an arbitrary class $\\mathbf{y}$ we have: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(\\mathbf{y} \\lvert \\mathbf{x}) &= \\frac{\\mathbb{P}(\\mathbf{y})\\mathbb{P}(\\mathbf{x} \\lvert \\mathbf{y})}{\\mathbb{P}(\\mathbf{x})} \\\\\n",
    "&= \\frac{\\mathbb{P}(\\mathbf{y})\\prod_{i=1}^p\\mathbb{P}(x_i\\lvert \\mathbf{y})}{\\mathbb{P}(\\mathbf{x})} \\propto \\mathbb{P}(\\mathbf{y})\\prod_{i=1}^p\\frac{1}{\\sqrt{2\\pi\\sigma_\\mathbf{y}^2}}exp\\left(-\\frac{(x_i-\\mu_\\mathbf{y})^2}{2\\sigma_\\mathbf{y}^2}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "where the parameters $\\sigma_\\mathbf{y}$ and $\\mu_\\mathbf{y}$ are estimated using maximum likelihood.\n",
    "\n",
    "The optimal classification $\\hat{\\mathbf{y}}$ is determined by the maximum likelihood estimator:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}} = \\underset{\\mathbf{y}}{\\arg\\max}\\mathbb{P}(\\mathbf{y})\\prod_{i=1}^p\\frac{1}{\\sqrt{2\\pi\\sigma_\\mathbf{y}^2}}exp\\left(-\\frac{(x_i-\\mu_\\mathbf{y})^2}{2\\sigma_\\mathbf{y}^2}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The *prior* $\\mathbb{P}(\\mathbf{y})$ can either be specified or automatically adjusted according to the heuristic distribution probability of each class in the training data. In my project the latter and default one in *sklearn* was adopted.\n",
    "\n",
    "- #### Logistic Regression\n",
    "Logistic regression with L2 penalty and the hyperparameter $C\\in(0, +\\infty)$ which controls how important the penalty should be, can be formulated as the following optimization problem:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\underset{w, b}{\\min}\\frac{1}{2}ww^T + C\\sum_{i=1}^nlog(exp(-y_i(X_iw^T + b)) + 1)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $X_1,..., X_n$ are X training samples.\n",
    "\n",
    "The probability that a certain sample $X_k$ will be classified as fraud is:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{P}(Y_k = 1) = \\frac{1}{1 + e^{-X_kw^T - b}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The algorithm is implemented with the default parameters from sklearn with the exception that *random_state* is set to 0, i.e. *(penalty=‚Äôl2‚Äô, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=0, solver=‚Äôliblinear‚Äô, max_iter=100, multi_class=‚Äôovr‚Äô, verbose=0, warm_start=False, n_jobs=1)*\n",
    "### Benchmark\n",
    "\n",
    "As mentioned before, I plan to benchmark my model against a neural network, which is trained by [Currie32 on Kaggle](https://www.kaggle.com/currie32/predicting-fraud-with-tensorflow) for exactly the same dataset. He claimed to have achieved a *recall* of 82.93% and a false positive rate of 0.10%, i.e. 82.92% of all fraudulent transitions were detected and 0.10% of the non-fraud transactions were falsely reported as fraud.\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "As already mentioned above, there is some pre-processing needed since the features 'Time' and 'Amount' are of much bigger scale than V1, ..., V28.\n",
    "\n",
    "- #### Time\n",
    "\n",
    "Clearly, the original feature 'Time' is the time eslapsed in second since the first transaction. Thus we can take the modulo 24x60x60 = 86,400, i.e. the total seconds in a day, to make it periodic. The result after the transformation is\n",
    "![](time_mod.png)\n",
    "\n",
    "It then needs to be rescaled using logarithmization and then normalization, which results in \n",
    "![](time_trans.png)\n",
    "\n",
    "The transformed feature 'Time' can now be used for discriminate between *fraud* and *non-fraud*, since they have very different distribution.\n",
    "\n",
    "- #### Amount\n",
    "The feature 'Amount' after logarithmization and normalization has the following distribution:\n",
    "![](amount_trans.png)\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- #### Shuffle and Split\n",
    "    - Normal Dataset\n",
    "    \n",
    "    The dataset is splited with 80% training and 20% testing data. In order to make the results reproducible, a random state was set.\n",
    "    ![](splitting.png)\n",
    "\n",
    "    - Undersampled Dataset\n",
    "    \n",
    "    Since the dataset is highly imbalanced, so is the training set, i.e. in the training data there is only a fraction of samples which are fraudulent. Which will presumably result in very low *recall* because the classifiers would not be 'well-educated' enough for identifying frauds. The easiest and most efficient way to tackle this problem is the so-called *undersampling*, i.e. the training set will be sampled in a way such that there are 50% *fraud* and 50% *non-fraud* training samples.\n",
    "    ![](undersample.png)\n",
    "    Thus 80% of the fraudulent transactions and the same amount of normal transactions, i.e. 789 samples for each class, will be our training set in the undersampling trial. The testing set which is created here will not be used for the purpose of testing. The classifiers trained on this undersampled training set will be tested on the same testing set from the *normal dataset* above.\n",
    "    \n",
    "- #### Pipeline\n",
    "I wrote a pipeline, such that I can train different classifiers and obtain the performance metrics on both the training and testing set in an on-line-code manner. The function has the inputs *classifier*, *training features*, *training labels*, *testing features* and *testing labels*. It starts with\n",
    "![](pipeline1.png)\n",
    "such that we can document the training and prediction time of the classifier. Then it calculates the performance metrics without AUC on the training set\n",
    "![](pipeline2.png)\n",
    "thereafter the performance metrics, all the *fpr* and *fpr* with different thresholds for the *ROC curve* and the *AUC* on the testing set\n",
    "![](pipeline3.png)\n",
    "at the end the metrics and the *ROC* as output\n",
    "![](pipeline4.png)\n",
    "\n",
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- _Has an initial solution been found and clearly reported?_\n",
    "- _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- _Are intermediate and final solutions clearly reported as the process is improved?_\n",
    "\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "### Model Evaluation\n",
    "- #### Normal Dataset\n",
    "    The 3 different classifiers are trained on the normal training set and testing on the normal testing set.\n",
    "    ![](normal_train.png)\n",
    "    Results are\n",
    "    ![](result_normal.png)\n",
    "    - ##### Decision Tree\n",
    "    We observe that, *decision tree classifier* has been highly overfitted and is the most computationally expensive. It has only achieve a *recall* score of 0.8119 and an *AUROC* of 0.9057. Its *false positive rate* is 0.0004, which is decent but not the best.\n",
    "    \n",
    "    - ##### Gaussian Naive Bayes\n",
    "    Gaussian naive Bayes offers the best *recall* score of 0.8515, with the price that the *false positive rate* being the highest, which is 0.0214.\n",
    "    \n",
    "    - ##### Logistic Regression\n",
    "    Logistic Regression has the best performance on the testing set regarding *AUROC* and *false positive rate* with the scores of 0.9783 and 0.0001 respectively. Unfortunately, the *recall* score is only as low as 0.6337. Which means only 63.37% of the fraudulent transactions were successfully detected.\n",
    "\n",
    "- #### Undersampled Dataset\n",
    "    The same classifiers are trained on the undersampled training set but testing on the normal testing set.\n",
    "    ![](under_train.png)\n",
    "    with the results\n",
    "    ![](result_under.png)\n",
    "    - ##### Decision Tree\n",
    "    This time *decision tree classifier* is still overfitted, however, it has achieve the best *recall* score of 0.9901 on the testing set. On the other hand, it has the worst performance on the testing set in terms of *false positive rate* and *AUROC*, 0.1057 and 0.9422 respectively, meaning that it has identified most frauds by paying the price of falsely categorizing most normal transactions.\n",
    "    \n",
    "    - ##### Gaussian Naive Bayes\n",
    "    Gaussian naive Bayes has achieved the best *false positive rate* of 0.0381 but also the worst *recall* score of 0.8713 on the testing set. It has an *AUROC* of 0.9618.\n",
    "    \n",
    "    - ##### Logistic Regression\n",
    "    Logistic regression performanced best on the testing set regarding *AUROC* with 0.9837. At the same time, its *false positive rate* is 0.0399, only a tick higher than that of Gaussian naive Bayes, while achieving a *recall* score of 0.9505.\n",
    "    \n",
    "- #### The Optimal Solution\n",
    "    After considering all the metrics comprehensively, I concluded that *logistic regression* with *undersampling* has the best performance on this particular problem.\n",
    "    \n",
    "### Model Validation\n",
    "Now I would like to validate *logistic regression* with 5 different *undersampled training sets* to see if it can be generalized. It is tested on the same normal testing set from before.\n",
    "![](cv.png)\n",
    "The results are:\n",
    "![](cv_result.png)\n",
    "It confirms that *logistic regression* offers very stable performances with different *undersampled training sets*.\n",
    "\n",
    "### Justification\n",
    "\n",
    "Comparing to the benchmark results, my model offers better *recall* with around 95.25% against 82.92%. However, it has much worse score on *false positive rate* with around 3.27% against 0.10%. I would consider this even with the benchmark results, because although *recall* is improved from 82.92% to 95.25%, the *false positive rate* increased more than 300%. Depending on how important *recall* is, we can now therefore adapt different strategies.\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "### Reflection\n",
    "\n",
    "In my opinion, the most interesting aspect of this project is that we can even achieve better results with less training data because of the skewness in the normal training set. With this project, I am convinced that *undersampling* is indeed an useful technique to deal with imbalanced classification problem.\n",
    "\n",
    "It is also very important that in the data exploration, we should consider which features need to be transformed such that it makes more sense for the algorithms.\n",
    "\n",
    "The major difficulty of this project is defining the metrics in a meaningful way. Some metrics, e.g. *accuracy* would not be suitable for imbalanced classification problem. While *AUROC* and *recall* would make more sense.\n",
    "\n",
    "### Improvement\n",
    "There is an improvement potential if we consider the feature 'Amount' not as a normal feature just like the other, but as a penalty for the mis-classification in the training phase. To be more concrete, we could use the untransformed original amount $\\alpha\\in[0,+\\inf]$ to re-define the optimization problem of *logistic regression* as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\underset{w, b}{\\min}\\frac{1}{2}ww^T + C\\sum_{i=1}^nlog(exp(-\\alpha_i\\cdot d(y_i, X_iw^T + b)) + 1)\n",
    "\\end{equation}\n",
    "$$\n",
    "i.e. transactions with bigger amount will be weighted more heavily, where $d(.,.)$ is some distance metrics.\n",
    "\n",
    "Another possibility would be that we fine tune the hyperparameter $C$ using cross validation. Or even we can try different optimization strategies to solve the above problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "5955268/AL6LRDFR": {
     "DOI": "10.1016/j.patrec.2005.10.010",
     "URL": "http://www.sciencedirect.com/science/article/pii/S016786550500303X",
     "abstract": "Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.",
     "accessed": {
      "day": 25,
      "month": 6,
      "year": 2018
     },
     "author": [
      {
       "family": "Fawcett",
       "given": "Tom"
      }
     ],
     "collection-title": "ROC Analysis in Pattern Recognition",
     "container-title": "Pattern Recognition Letters",
     "container-title-short": "Pattern Recognition Letters",
     "id": "5955268/AL6LRDFR",
     "issue": "8",
     "issued": {
      "day": 1,
      "month": 6,
      "year": 2006
     },
     "journalAbbreviation": "Pattern Recognition Letters",
     "page": "861-874",
     "page-first": "861",
     "title": "An introduction to ROC analysis",
     "type": "article-journal",
     "volume": "27"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
